# DataOps Pipeline: DBT + Airflow + CI/CD

[![Deploy to Development](https://github.com/qnm1903/dbt_airflow-dataops/actions/workflows/deploy-dev.yml/badge.svg)](https://github.com/qnm1903/dbt_airflow-dataops/actions/workflows/deploy-dev.yml)
[![Deploy to Production](https://github.com/qnm1903/dbt_airflow-dataops/actions/workflows/deploy-prod.yml/badge.svg)](https://github.com/qnm1903/dbt_airflow-dataops/actions/workflows/deploy-prod.yml)

**Final Year DataOps Project**
*Automated data transformation pipeline with DBT, Airflow orchestration, and GitHub Actions CI/CD*

---

## üìã Table of Contents

- [Project Overview](#-project-overview)
- [Features](#-features)
- [Architecture](#-architecture)
- [Tech Stack](#-tech-stack)
- [Quick Start](#-quick-start)
- [Project Structure](#-project-structure)
- [Data Pipeline](#-data-pipeline)
- [CI/CD Workflows](#-cicd-workflows)
- [Deployment](#-deployment)
- [Monitoring & Alerts](#-monitoring--alerts)
- [Development Guide](#-development-guide)
- [Troubleshooting](#-troubleshooting)

---

## üéØ Project Overview

This project implements a production-grade DataOps pipeline that automates data transformation workflows using industry-standard tools. The pipeline extracts data from SQL Server (AdventureWorks 2014), transforms it through bronze/silver/gold layers using DBT, orchestrates execution with Apache Airflow, and deploys automatically via GitHub Actions CI/CD.

### Business Use Case

Transform raw sales, customer, and product data from AdventureWorks into analytics-ready datasets for business intelligence and reporting.

### Learning Outcomes

- ‚úÖ Production-grade data pipeline implementation
- ‚úÖ DataOps principles: CI/CD, automated testing, monitoring
- ‚úÖ Containerization with Docker Compose
- ‚úÖ Version control and collaboration with Git/GitHub
- ‚úÖ Data quality frameworks and testing strategies
- ‚úÖ Infrastructure as Code (IaC) practices

---

## ‚ú® Features

### Core Features
- **üìä 3-Layer Data Architecture:** Bronze (raw) -> Silver (cleaned) -> Gold (aggregated)
- **üîÑ Automated Orchestration:** Airflow DAGs with dependency management
- **‚úÖ Data Quality Testing:** 126+ automated tests (schema, custom, relationships)
- **üöÄ CI/CD Automation:** GitHub Actions for testing and deployment
- **üì¶ Containerized Infrastructure:** Docker Compose for all services
- **üìà Source Freshness Monitoring:** Track data staleness
- **üîî Slack Notifications:** Real-time alerts for pipeline failures/success
- **üìù Auto-generated Documentation:** DBT docs with data lineage

### Advanced Features
- **Environment-specific Deployments:** Separate dev/prod configurations
- **Rollback Capability:** Safe deployment with failure handling
- **Pre-commit Hooks:** Code quality checks before commits
- **Self-hosted GitHub Runner:** Deploy to local infrastructure

---

## üèóÔ∏è Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    GitHub (Version Control)                   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ
‚îÇ  ‚îÇ   develop    ‚îÇ‚îÄ‚îÄ‚îÄ‚Üí‚îÇ  Pull Request ‚îÇ‚îÄ‚îÄ‚Üí‚îÇ     main     ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ   branch     ‚îÇ    ‚îÇ   (CI Tests)  ‚îÇ   ‚îÇ   branch     ‚îÇ     ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ
‚îÇ         ‚îÇ                                         ‚îÇ           ‚îÇ
‚îÇ         ‚îÇ merge                                   ‚îÇ merge     ‚îÇ
‚îÇ         ‚ñº                                         ‚ñº           ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ
‚îÇ  ‚îÇDeploy to Dev ‚îÇ                        ‚îÇDeploy to Prod‚îÇ     ‚îÇ
‚îÇ  ‚îÇ(Auto-trigger)‚îÇ                        ‚îÇ(Auto-trigger)‚îÇ     ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ                         ‚îÇ
                           ‚ñº                         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              Local Infrastructure (Docker Compose)            ‚îÇ
‚îÇ                                                               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  SQL Server    ‚îÇ   ‚îÇ   PostgreSQL   ‚îÇ   ‚îÇ   Airflow      ‚îÇ ‚îÇ
‚îÇ  ‚îÇ (AdventureWorks‚îÇ‚óÑ‚îÄ‚îÄ‚îÇ   (Metadata)   ‚îÇ‚óÑ‚îÄ‚îÄ‚îÇ  Webserver &   ‚îÇ ‚îÇ
‚îÇ  ‚îÇ     2014)      ‚îÇ   ‚îÇ                ‚îÇ   ‚îÇ   Scheduler    ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ          ‚ñ≤                                         ‚îÇ          ‚îÇ
‚îÇ          ‚îÇ                                         ‚îÇ          ‚îÇ
‚îÇ          ‚îÇ                                         ‚ñº          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ
‚îÇ  ‚îÇ                   DBT Container                     ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  Bronze  ‚îÇ‚îÄ‚îÄ‚Üí‚îÇ  Silver  ‚îÇ‚îÄ‚îÄ‚Üí‚îÇ   Gold    ‚îÇ        ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  Layer   ‚îÇ   ‚îÇ  Layer   ‚îÇ   ‚îÇ  Layer    ‚îÇ        ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ(Raw Data)‚îÇ   ‚îÇ(Cleaned) ‚îÇ   ‚îÇ(Analytics)‚îÇ        ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îÇ      ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Data Flow

```
SQL Server (Source)
    ‚îÇ
    ‚îú‚îÄ‚Üí Bronze Layer (4 models)
    ‚îÇ    ‚Ä¢ brnz_customers
    ‚îÇ    ‚Ä¢ brnz_products
    ‚îÇ    ‚Ä¢ brnz_sales_orders
    ‚îÇ    ‚Ä¢ brnz_example
    ‚îÇ
    ‚îú‚îÄ‚Üí Silver Layer (4 models)
    ‚îÇ    ‚Ä¢ slvr_customers (cleaned & enriched)
    ‚îÇ    ‚Ä¢ slvr_products (profit margins)
    ‚îÇ    ‚Ä¢ slvr_sales_orders (order details)
    ‚îÇ    ‚Ä¢ slvr_order_summary (aggregated)
    ‚îÇ
    ‚îî‚îÄ‚Üí Gold Layer (3 models)
         ‚Ä¢ gld_customer_metrics (lifetime value, segments)
         ‚Ä¢ gld_product_performance (top products, trends)
         ‚Ä¢ gld_sales_summary (revenue, KPIs)
```

---

## üõ†Ô∏è Tech Stack

| Component | Technology | Purpose |
|-----------|-----------|---------|
| **Orchestration** | Apache Airflow 2.x | Workflow scheduling and monitoring |
| **Transformation** | DBT 1.9.0 | SQL-based data modeling |
| **Database (Source)** | SQL Server 2019 | AdventureWorks sample data |
| **Database (Metadata)** | PostgreSQL 13 | Airflow metadata storage |
| **Containerization** | Docker & Docker Compose | Service orchestration |
| **Version Control** | Git & GitHub | Code collaboration |
| **CI/CD** | GitHub Actions | Automated testing & deployment |
| **Language** | SQL, Python, YAML | Data models, scripts, configs |
| **ODBC Driver** | ODBC Driver 17 for SQL Server | Database connectivity |
| **Notifications** | Slack API | Pipeline alerts |

---

## üöÄ Quick Start

### Prerequisites

- **Docker Desktop** (Windows/Mac) or Docker Engine + Docker Compose (Linux)
- **Git** for version control
- **GitHub account** with repository access
- **4GB+ RAM** available for containers
- **Windows 10/11** with PowerShell 5.1+ (for self-hosted runner)

### Installation Steps

**1. Clone the Repository**

```bash
git clone https://github.com/qnm1903/dbt_airflow-dataops.git
cd dbt_airflow-dataops
```

**2. Set Environment Variables**

Create `.env` file in project root:

```bash
# Airflow
AIRFLOW_FERNET_KEY=your_fernet_key_here

# Slack (Optional)
SLACK_WEBHOOK_URL=https://hooks.slack.com/services/YOUR/WEBHOOK/URL
```

Generate Fernet key:
```bash
python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"
```

**3. Start All Services**

```bash
docker compose up -d
```

This will start:
- SQL Server (port 1433)
- PostgreSQL (port 5432)
- Airflow Webserver (port 8080)
- Airflow Scheduler
- DBT Container

**4. Restore AdventureWorks Database**

```bash
docker exec dbt_airflow-dataops-sqlserver-1 /opt/mssql-tools/bin/sqlcmd -S localhost -U sa -P "YourStrong@Passw0rd" -Q "RESTORE DATABASE AdventureWorks2014 FROM DISK = '/tmp/AdventureWorks2014.bak' WITH MOVE 'AdventureWorks2014_Data' TO '/var/opt/mssql/data/AdventureWorks2014_Data.mdf', MOVE 'AdventureWorks2014_Log' TO '/var/opt/mssql/data/AdventureWorks2014_Log.ldf', REPLACE"
```

**5. Access Airflow UI**

- Open browser: http://localhost:8080
- Username: `admin`
- Password: `admin`

**6. Trigger Pipeline**

In Airflow UI:
1. Find DAG: `dbt_transform_pipeline`
2. Toggle ON (enable DAG)
3. Click ‚ñ∂Ô∏è (trigger DAG)

---

## üìÅ Project Structure

```
dbt_airflow-dataops/
‚îú‚îÄ‚îÄ .github/
‚îÇ   ‚îî‚îÄ‚îÄ workflows/
‚îÇ       ‚îú‚îÄ‚îÄ deploy-dev.yml          # Dev deployment workflow
‚îÇ       ‚îú‚îÄ‚îÄ deploy-prod.yml         # Prod deployment workflow
‚îÇ       ‚îî‚îÄ‚îÄ dbt-ci.yml              # DBT CI Pipeline
‚îú‚îÄ‚îÄ airflow/
‚îÇ   ‚îú‚îÄ‚îÄ dags/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dbt_dag.py             # Main DBT pipeline DAG
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dbt_hourly_dag.py      # Hourly schedule DAG
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dbt_full_refresh_dag.py # Full refresh DAG
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ alerting.py        # Slack notifications
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ logging_utils.py   # Custom logging
‚îÇ   ‚îú‚îÄ‚îÄ logs/                       # Airflow logs
‚îÇ   ‚îú‚îÄ‚îÄ plugins/                    # Custom plugins
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile                  # Airflow container image
‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt            # Python dependencies
‚îú‚îÄ‚îÄ dbt/
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sources.yml            # Source definitions
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ bronze/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ brnz_customers.sql
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ brnz_products.sql
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ brnz_sales_orders.sql
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ schema.yml         # Bronze tests
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ silver/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ slvr_customers.sql
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ slvr_products.sql
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ slvr_sales_orders.sql
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ slvr_order_summary.sql
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ schema.yml         # Silver tests
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ gold/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ gld_customer_metrics.sql
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ gld_product_performance.sql
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ gld_sales_summary.sql
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ schema.yml         # Gold tests
‚îÇ   ‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ generic/               # Reusable test macros
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ data_quality/          # Custom data tests
‚îÇ   ‚îú‚îÄ‚îÄ dbt_project.yml            # DBT project config
‚îÇ   ‚îú‚îÄ‚îÄ profiles.yml               # Connection profiles
‚îÇ   ‚îú‚îÄ‚îÄ packages.yml               # DBT packages
‚îÇ   ‚îî‚îÄ‚îÄ Dockerfile                 # DBT container image
‚îú‚îÄ‚îÄ sqlserver/
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile                 # SQL Server image
‚îÇ   ‚îî‚îÄ‚îÄ restore_db.sh              # DB restore script
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îú‚îÄ‚îÄ images/data-lineage.png    # Image of data lineage
‚îÇ   ‚îú‚îÄ‚îÄ DEPLOYMENT_RUNBOOK.md      # Deployment guide
‚îÇ   ‚îî‚îÄ‚îÄ ROLLBACK_PROCEDURES.md     # Rollback steps
‚îú‚îÄ‚îÄ docker-compose.yml             # Service orchestration
‚îú‚îÄ‚îÄ README.md                      # This file
‚îî‚îÄ‚îÄ DATAOPS_PROJECT_REQUIREMENT.md # Project requirements

```

---

## üìä Data Pipeline

### Bronze Layer (Raw Data)

**Purpose:** Extract and lightly clean source data

| Model | Source Table | Records | Purpose |
|-------|-------------|---------|---------|
| `brnz_customers` | Person.Person + Sales.Customer | 19,820 | Customer master data |
| `brnz_products` | Production.Product | 504 | Product catalog |
| `brnz_sales_orders` | Sales.SalesOrderHeader + Detail | 121,317 | Order transactions |

**Transformations:**
- Column renaming for consistency
- Data type casting
- Null handling
- Timestamp tracking (`bronze_created_at`)

### Silver Layer (Cleaned & Business Logic)

**Purpose:** Apply business rules and create analysis-ready datasets

| Model | Dependencies | Key Transformations |
|-------|-------------|-------------------|
| `slvr_customers` | brnz_customers | Customer type classification, full name |
| `slvr_products` | brnz_products | Profit margin calculation, category grouping |
| `slvr_sales_orders` | brnz_sales_orders | Revenue calculation, order categorization |
| `slvr_order_summary` | slvr_sales_orders | Order aggregation by customer |

**Key Metrics:**
- Customer lifetime calculations
- Product profitability analysis
- Order value categorization (LOW/MEDIUM/HIGH/PREMIUM)
- Time-based aggregations

### Gold Layer (Analytics-Ready Marts)

**Purpose:** Business-ready aggregated datasets for BI tools

| Model | Use Case | Key Metrics |
|-------|----------|------------|
| `gld_customer_metrics` | Customer analytics | Lifetime value, total orders, avg order value, segments |
| `gld_product_performance` | Product analytics | Revenue, units sold, margin %, top performers |
| `gld_sales_summary` | Executive dashboard | Total revenue, order count, avg order value, trends |

### Data Lineage Visualization

The project includes interactive data lineage visualization powered by DBT's built-in documentation.

**View Lineage Locally:**

```bash
cd dbt
dbt docs generate
dbt docs serve --port 8080
# Open http://localhost:8080 in browser
```

**Lineage Features:**
- üìä Interactive DAG visualization showing model dependencies
- üîó Visual representation of Bronze -> Silver -> Gold flow
- üìù Column-level lineage tracking
- üéØ Click any model to see upstream/downstream dependencies

**Example Lineage:**

![Data Lineage Visualization](docs/images/data-lineage.png)

---

## üîÑ CI/CD Workflows

### Workflow 1: Deploy to Development

**Trigger:** Push to `develop` branch with changes in `dbt/` or `airflow/`

**Steps:**
1. ‚úÖ Checkout code
2. ‚úÖ Install DBT dependencies (`dbt-sqlserver==1.9.0`)
3. ‚úÖ Configure DBT profiles (dev environment)
4. ‚úÖ Install DBT packages (`dbt deps`)
5. ‚úÖ Run DBT models (`dbt run --target dev`)
6. ‚úÖ Execute data quality tests (`dbt test --target dev`)
7. ‚úÖ Generate documentation (`dbt docs generate`)
8. ‚úÖ Send notification (success/failure)

**Configuration:**
- **Runner:** `self-hosted` (local Windows machine)
- **Target:** Development database on localhost:1433
- **Credentials:** GitHub Secrets (`DEV_SQL_*`)

### Workflow 2: Deploy to Production

**Trigger:** Push to `main` branch (merge from develop)

**Steps:**
1. ‚úÖ Pre-deployment validation
   - Check for uncommitted changes
   - Validate DBT project structure
   - Run smoke tests
2. ‚úÖ Create backup snapshot (database state)
3. ‚úÖ Deploy DBT models to production
4. ‚úÖ Run comprehensive tests
5. ‚úÖ Health check (verify deployment success)
6. ‚úÖ **Rollback if tests fail** (automatic)
7. ‚úÖ Send deployment notification

**Configuration:**
- **Runner:** `self-hosted`
- **Target:** Production database (separate instance recommended)
- **Credentials:** GitHub Secrets (`PROD_SQL_*`)
- **Approval:** Optional manual approval before prod deploy
---

## üöÄ Deployment

### Manual Deployment (Local Testing)

**Run specific layer:**
```bash
# Run only Bronze layer
docker exec dbt_airflow-dataops-dbt-1 dbt run --select bronze --profiles-dir /usr/app/dbt

# Run only Silver layer
docker exec dbt_airflow-dataops-dbt-1 dbt run --select silver --profiles-dir /usr/app/dbt

# Run only Gold layer
docker exec dbt_airflow-dataops-dbt-1 dbt run --select gold --profiles-dir /usr/app/dbt
```

**Run specific model:**
```bash
docker exec dbt_airflow-dataops-dbt-1 dbt run --select slvr_customers --profiles-dir /usr/app/dbt
```

**Run tests:**
```bash
# All tests
docker exec dbt_airflow-dataops-dbt-1 dbt test --profiles-dir /usr/app/dbt

# Tests for specific model
docker exec dbt_airflow-dataops-dbt-1 dbt test --select slvr_customers --profiles-dir /usr/app/dbt
```

**Check source freshness:**
```bash
docker exec dbt_airflow-dataops-dbt-1 dbt source freshness --profiles-dir /usr/app/dbt
```

### Automated Deployment (GitHub Actions)

**Deploy to Development:**
1. Create feature branch: `git checkout -b feature/your-feature`
2. Make changes to DBT models or Airflow DAGs
3. Commit and push: `git push origin feature/your-feature`
4. Create Pull Request to `develop`
5. After PR approval, merge to `develop`
6. Check Slack for deployment notification

**Deploy to Production:**
1. Merge `develop` -> `main` via Pull Request
3. Monitor deployment in GitHub Actions tab
4. Check Slack for deployment notification

### Rollback Procedures

**Automated Rollback (Production Only):**
- If tests fail after deployment, workflow automatically rolls back to previous state
- Rollback includes restoring database snapshot (if configured)

**Manual Rollback:**
```bash
# Option 1: Revert to previous commit
git revert HEAD
git push

# Option 2: Reset to specific commit
git reset --hard <commit-hash>
git push --force

# Option 3: Re-run previous successful deployment
# Go to GitHub Actions -> Find last successful run -> Re-run jobs
```

See [ROLLBACK_PROCEDURES.md](docs/ROLLBACK_PROCEDURES.md) for detailed steps.

---

## üìà Monitoring & Alerts

### Airflow Monitoring

**Access Airflow UI:** http://localhost:8080

**Key Metrics to Monitor:**
- DAG success rate
- Task execution time
- Failed tasks (red tiles)
- Queued tasks
- Source freshness warnings

**DAG Schedule:**
- `dbt_transform_pipeline`: Daily at 06:00 UTC
- `dbt_transform_hourly`: Every hour
- `dbt_full_refresh_dag`: Weekly (Sunday 02:00 UTC)

### Slack Notifications

**Automatic Alerts for:**
- ‚ùå Pipeline failures with error details
- ‚úÖ Successful pipeline completion
- ‚ö†Ô∏è Source freshness warnings
- üöÄ Deployment notifications (dev/prod)
- üîÑ Rollback events

**Configure Slack:**
1. Create Slack app: https://api.slack.com/apps
2. Enable Incoming Webhooks
3. Copy webhook URL
4. Add to `.env`: `SLACK_WEBHOOK_URL=https://hooks.slack.com/...`
5. Restart Airflow: `docker compose restart airflow-webserver airflow-scheduler`

---

## üõ†Ô∏è Development Guide

### Adding New DBT Model

**1. Create SQL file:**
```bash
# Create in appropriate layer folder
touch dbt/models/silver/slvr_new_model.sql
```

**2. Write transformation:**
```sql
{{
    config(
        materialized='table',
        schema='silver'
    )
}}

select
    column1,
    column2,
    {{ dbt_utils.generate_surrogate_key(['column1', 'column2']) }} as surrogate_key
from {{ ref('brnz_source_model') }}
where condition = true
```

**3. Add tests in schema.yml:**
```yaml
models:
  - name: slvr_new_model
    description: "Model description"
    columns:
      - name: surrogate_key
        description: "Unique identifier"
        tests:
          - unique
          - not_null
```

**4. Test locally:**
```bash
docker exec dbt_airflow-dataops-dbt-1 dbt run --select slvr_new_model --profiles-dir /usr/app/dbt
docker exec dbt_airflow-dataops-dbt-1 dbt test --select slvr_new_model --profiles-dir /usr/app/dbt
```

**5. Commit and push:**
```bash
git add dbt/models/silver/
git commit -m "feat: add slvr_new_model for customer analysis"
git push origin feature/new-model
```

### Adding New Airflow DAG

**1. Define DAG:**
```python
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.bash import BashOperator

default_args = {
    "owner": "data_engineering",
    "retries": 1,
    "retry_delay": timedelta(minutes=5),
}

dag = DAG(
    "my_new_dag",
    default_args=default_args,
    description="Description",
    schedule_interval="@daily",
    start_date=datetime(2024, 1, 1),
    catchup=False,
    tags=["custom"],
)

task = BashOperator(
    task_id="run_something",
    bash_command="echo 'Hello World'",
    dag=dag,
)
```

**2. Test DAG:**
```bash
# Check for syntax errors
docker exec dbt_airflow-dataops-airflow-webserver-1 airflow dags list

# Test task execution
docker exec dbt_airflow-dataops-airflow-webserver-1 airflow tasks test my_new_dag run_something 2024-01-01
```

**3. Deploy:**
- Airflow automatically detects new DAG files
- Refresh Airflow UI to see new DAG
- Toggle DAG on and trigger manually for testing

### Git Workflow

**Feature Development:**
```bash
# 1. Create feature branch from develop
git checkout develop
git pull origin develop
git checkout -b feature/your-feature-name

# 2. Make changes and commit frequently
git add .
git commit -m "feat: add new customer segmentation model"

# 3. Push to remote
git push origin feature/your-feature-name

# 4. Create Pull Request to develop

# 5. After PR approval, merge to develop
```

**Production Release:**
```bash
# 1. Create release branch
git checkout develop
git checkout -b release/v1.0.0

# 2. Final testing and bug fixes

# 3. Merge to main
git checkout main
git merge release/v1.0.0
git push origin main

# 4. Tag release
git tag -a v1.0.0 -m "Release version 1.0.0"
git push origin v1.0.0
```

---

## üêõ Troubleshooting

### Issue: Containers won't start

**Symptoms:**
```
ERROR: Container exited with code 1
```

**Solutions:**
```bash
# Check logs
docker compose logs

# Restart specific service
docker compose restart <service-name>

# Full restart
docker compose down
docker compose up -d

# Check system resources
docker stats
```

### Issue: SQL Server connection timeout

**Symptoms:**
```
Login timeout expired
Cannot connect to SQL Server
```

**Solutions:**
```bash
# 1. Verify SQL Server is running
docker ps | grep sqlserver

# 2. Check SQL Server logs
docker logs dbt_airflow-dataops-sqlserver-1

# 3. Test connection
docker exec dbt_airflow-dataops-sqlserver-1 /opt/mssql-tools/bin/sqlcmd -S localhost -U sa -P "YourStrong@Passw0rd" -Q "SELECT @@VERSION"

# 4. Verify database exists
docker exec dbt_airflow-dataops-sqlserver-1 /opt/mssql-tools/bin/sqlcmd -S localhost -U sa -P "YourStrong@Passw0rd" -Q "SELECT name FROM sys.databases"

# 5. If AdventureWorks missing, restore it again
# (See Quick Start step 4)
```

### Issue: DBT models fail with "profile not found"

**Symptoms:**
```
Could not find profile named 'dbt_airflow_project'
```

**Solutions:**
```bash
# 1. Check profiles.yml exists
docker exec dbt_airflow-dataops-dbt-1 cat /usr/app/dbt/profiles.yml

# 2. Verify connection settings
docker exec dbt_airflow-dataops-dbt-1 dbt debug --profiles-dir /usr/app/dbt

# 3. Test connection manually
docker exec dbt_airflow-dataops-dbt-1 dbt run --select brnz_customers --profiles-dir /usr/app/dbt
```

### Issue: Airflow scheduler not running

**Symptoms:**
```
The scheduler does not appear to be running
DAGs not scheduling
```

**Solutions:**
```bash
# 1. Check if scheduler container is running
docker ps | grep scheduler

# 2. Start scheduler if stopped
docker compose up -d airflow-scheduler

# 3. Check scheduler logs
docker logs dbt_airflow-dataops-airflow-scheduler-1

# 4. Restart scheduler
docker compose restart airflow-scheduler
```

### Issue: GitHub Actions workflow fails

**Symptoms:**
```
Workflow run failed
Tests pass locally but fail in CI
```

**Solutions:**
1. **Check GitHub Secrets:**
   - Go to repo Settings -> Secrets -> Actions
   - Verify `DEV_SQL_*` and `PROD_SQL_*` secrets exist
   - Ensure self-hosted runner is online

2. **Check workflow logs:**
   - GitHub repo -> Actions tab
   - Click failed workflow run
   - Expand failed step to see error details

3. **Common fixes:**
   ```yaml
   # Ensure secrets are properly referenced
   server: ${{ secrets.DEV_SQL_SERVER }}

   # Add SSL bypass for local SQL Server
   encrypt: false
   trust_cert: true
   ```

4. **Test workflow locally:**
   ```bash
   # Install act (GitHub Actions local runner)
   # Run workflow locally to debug
   act -j deploy
   ```

### Issue: DBT tests fail on first run

**Symptoms:**
```
ERROR: Invalid object name 'gold.gld_customer_metrics'
Tests reference tables that don't exist yet
```

**Solution:**
```bash
# Run full pipeline once to build all layers
docker exec dbt_airflow-dataops-dbt-1 dbt run --profiles-dir /usr/app/dbt

# Then run tests
docker exec dbt_airflow-dataops-dbt-1 dbt test --profiles-dir /usr/app/dbt

# Or trigger complete DAG in Airflow UI
```

### Issue: Port already in use

**Symptoms:**
```
ERROR: Port 8080 is already allocated
ERROR: Port 1433 is already allocated
```

**Solutions:**
```bash
# Option 1: Stop conflicting service
# Windows:
netstat -ano | findstr :8080
taskkill /PID <PID> /F

# Option 2: Change port in docker-compose.yml
# Edit ports section:
ports:
  - "8081:8080"  # Use 8081 instead of 8080
```

### Getting Help

**Resources:**
- [DBT Documentation](https://docs.getdbt.com/)
- [Airflow Documentation](https://airflow.apache.org/docs/)
- [Docker Documentation](https://docs.docker.com/)
- [Project Issues](https://github.com/qnm1903/dbt_airflow-dataops/issues)

**Contact:**
- Create GitHub Issue with:
  - Error message (full stack trace)
  - Steps to reproduce
  - Environment details (`docker version`, OS)
  - Relevant logs
---

## üìö Additional Documentation

- **[DEPLOYMENT_RUNBOOK.md](docs/DEPLOYMENT_RUNBOOK.md)** - Step-by-step deployment guide
- **[ROLLBACK_PROCEDURES.md](docs/ROLLBACK_PROCEDURES.md)** - Emergency rollback steps
- **[ARCHITECTURE.md](ARCHITECTURE.md)** - Detailed architecture explanation
- **[DBT_ETL_GUIDE.md](DBT_ETL_GUIDE.md)** - DBT modeling best practices
- **[DATAOPS_GUIDE.md](DATAOPS_GUIDE.md)** - DataOps principles and workflows
- **[TROUBLESHOOTING.md](TROUBLESHOOTING.md)** - Extended troubleshooting guide

---

## üìù License

This project is developed for educational purposes as part of a university DataOps assignment.

---

## üôè Acknowledgments

- **AdventureWorks Database** - Microsoft sample database
- **DBT Community** - Documentation and packages
- **Airflow Community** - Orchestration platform
- **Course Instructors** - Project guidance and requirements
- **Open Source Contributors** - Tools and libraries used

**Live Pipeline:**
1. Clone repository
2. Run `docker compose up -d`
3. Access Airflow: http://localhost:8080
4. Trigger `dbt_transform_pipeline` DAG
5. Watch data flow through Bronze -> Silver -> Gold layers

---

**Questions? Found a bug? Open an issue!**

[![Issues](https://img.shields.io/github/issues/qnm1903/dbt_airflow-dataops)](https://github.com/qnm1903/dbt_airflow-dataops/issues)
[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg)](https://github.com/qnm1903/dbt_airflow-dataops/pulls)
